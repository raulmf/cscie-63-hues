{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction's accuracy depending on the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "batch_size_array = []\n",
    "accuracy_array = []\n",
    "execution_time_array = []\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder mnist\n",
    "mnist = input_data.read_data_sets('./mnist', one_hot=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.51453666064\n",
      "Average loss epoch 1: 0.353858619801\n",
      "Average loss epoch 2: 0.327117213843\n",
      "Average loss epoch 3: 0.313448230271\n",
      "Average loss epoch 4: 0.30424589073\n",
      "Average loss epoch 5: 0.298021698803\n",
      "Average loss epoch 6: 0.292721647128\n",
      "Average loss epoch 7: 0.288947734166\n",
      "Average loss epoch 8: 0.285468569932\n",
      "Average loss epoch 9: 0.282468648653\n",
      "Average loss epoch 10: 0.280053560026\n",
      "Average loss epoch 11: 0.278254314146\n",
      "Average loss epoch 12: 0.276203185275\n",
      "Average loss epoch 13: 0.274515085229\n",
      "Average loss epoch 14: 0.272890597633\n",
      "Average loss epoch 15: 0.271541728993\n",
      "Average loss epoch 16: 0.270309704783\n",
      "Average loss epoch 17: 0.269229754018\n",
      "Average loss epoch 18: 0.268038138859\n",
      "Average loss epoch 19: 0.26693008437\n",
      "Average loss epoch 20: 0.266104395678\n",
      "Average loss epoch 21: 0.265063562233\n",
      "Average loss epoch 22: 0.264341878011\n",
      "Average loss epoch 23: 0.263687922481\n",
      "Average loss epoch 24: 0.262617497842\n",
      "Average loss epoch 25: 0.261678206002\n",
      "Average loss epoch 26: 0.261270158285\n",
      "Average loss epoch 27: 0.260589568073\n",
      "Average loss epoch 28: 0.259962876072\n",
      "Average loss epoch 29: 0.259651570843\n",
      "Total time: 88.2807481289 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9262\n",
      "[88.28086519241333]\n",
      "[8]\n",
      "[0.92620000000000002]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 8\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tbatch_size_array.append(batch_size)\n",
    "\taccuracy_array.append(total_correct_preds/mnist.test.num_examples)\n",
    "\tprint execution_time_array\n",
    "\tprint batch_size_array\n",
    "\tprint accuracy_array\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.00850171781\n",
      "Average loss epoch 1: 0.56870411692\n",
      "Average loss epoch 2: 0.484888928039\n",
      "Average loss epoch 3: 0.444000070427\n",
      "Average loss epoch 4: 0.418868402316\n",
      "Average loss epoch 5: 0.401101127093\n",
      "Average loss epoch 6: 0.387640372535\n",
      "Average loss epoch 7: 0.377223302342\n",
      "Average loss epoch 8: 0.368860641687\n",
      "Average loss epoch 9: 0.361806076276\n",
      "Average loss epoch 10: 0.355682304474\n",
      "Average loss epoch 11: 0.350392118686\n",
      "Average loss epoch 12: 0.345650885571\n",
      "Average loss epoch 13: 0.341589188912\n",
      "Average loss epoch 14: 0.337917564651\n",
      "Average loss epoch 15: 0.334667555485\n",
      "Average loss epoch 16: 0.331613609327\n",
      "Average loss epoch 17: 0.328859893811\n",
      "Average loss epoch 18: 0.326415444463\n",
      "Average loss epoch 19: 0.324030694536\n",
      "Average loss epoch 20: 0.321852518469\n",
      "Average loss epoch 21: 0.319766572264\n",
      "Average loss epoch 22: 0.317937117135\n",
      "Average loss epoch 23: 0.316149675961\n",
      "Average loss epoch 24: 0.31446514996\n",
      "Average loss epoch 25: 0.312955318225\n",
      "Average loss epoch 26: 0.311271421256\n",
      "Average loss epoch 27: 0.31000257136\n",
      "Average loss epoch 28: 0.308541701883\n",
      "Average loss epoch 29: 0.307305363242\n",
      "Total time: 22.7510621548 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Test:39\n",
      "Test:40\n",
      "Test:41\n",
      "Test:42\n",
      "Test:43\n",
      "Test:44\n",
      "Test:45\n",
      "Test:46\n",
      "Test:47\n",
      "Test:48\n",
      "Test:49\n",
      "Test:50\n",
      "Test:51\n",
      "Test:52\n",
      "Test:53\n",
      "Test:54\n",
      "Test:55\n",
      "Test:56\n",
      "Test:57\n",
      "Test:58\n",
      "Test:59\n",
      "Test:60\n",
      "Test:61\n",
      "Test:62\n",
      "Test:63\n",
      "Test:64\n",
      "Test:65\n",
      "Test:66\n",
      "Test:67\n",
      "Test:68\n",
      "Test:69\n",
      "Test:70\n",
      "Test:71\n",
      "Test:72\n",
      "Test:73\n",
      "Test:74\n",
      "Test:75\n",
      "Test:76\n",
      "Test:77\n",
      "Test:78\n",
      "Test:79\n",
      "Test:80\n",
      "Test:81\n",
      "Test:82\n",
      "Test:83\n",
      "Test:84\n",
      "Test:85\n",
      "Test:86\n",
      "Test:87\n",
      "Test:88\n",
      "Test:89\n",
      "Test:90\n",
      "Test:91\n",
      "Test:92\n",
      "Test:93\n",
      "Test:94\n",
      "Test:95\n",
      "Test:96\n",
      "Test:97\n",
      "Test:98\n",
      "Test:99\n",
      "Test:100\n",
      "Test:101\n",
      "Test:102\n",
      "Test:103\n",
      "Test:104\n",
      "Test:105\n",
      "Test:106\n",
      "Test:107\n",
      "Test:108\n",
      "Test:109\n",
      "Test:110\n",
      "Test:111\n",
      "Test:112\n",
      "Test:113\n",
      "Test:114\n",
      "Test:115\n",
      "Test:116\n",
      "Test:117\n",
      "Test:118\n",
      "Test:119\n",
      "Test:120\n",
      "Test:121\n",
      "Test:122\n",
      "Test:123\n",
      "Test:124\n",
      "Test:125\n",
      "Test:126\n",
      "Test:127\n",
      "Test:128\n",
      "Test:129\n",
      "Test:130\n",
      "Test:131\n",
      "Test:132\n",
      "Test:133\n",
      "Test:134\n",
      "Test:135\n",
      "Test:136\n",
      "Test:137\n",
      "Test:138\n",
      "Test:139\n",
      "Test:140\n",
      "Test:141\n",
      "Test:142\n",
      "Test:143\n",
      "Test:144\n",
      "Test:145\n",
      "Test:146\n",
      "Test:147\n",
      "Test:148\n",
      "Test:149\n",
      "Test:150\n",
      "Test:151\n",
      "Test:152\n",
      "Test:153\n",
      "Test:154\n",
      "Test:155\n",
      "Accuracy 0.9166\n",
      "[88.28086519241333, 22.751082181930542]\n",
      "[8, 64]\n",
      "[0.92620000000000002, 0.91659999999999997]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tbatch_size_array.append(batch_size)\n",
    "\taccuracy_array.append(total_correct_preds/mnist.test.num_examples)\n",
    "\tprint execution_time_array\n",
    "\tprint batch_size_array\n",
    "\tprint accuracy_array\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.28842262719\n",
      "Average loss epoch 1: 0.732656761205\n",
      "Average loss epoch 2: 0.60027062233\n",
      "Average loss epoch 3: 0.536507627\n",
      "Average loss epoch 4: 0.497711298766\n",
      "Average loss epoch 5: 0.470890493312\n",
      "Average loss epoch 6: 0.451249177709\n",
      "Average loss epoch 7: 0.435931822706\n",
      "Average loss epoch 8: 0.423389143321\n",
      "Average loss epoch 9: 0.412992481655\n",
      "Average loss epoch 10: 0.404197049898\n",
      "Average loss epoch 11: 0.396646488449\n",
      "Average loss epoch 12: 0.390255042549\n",
      "Average loss epoch 13: 0.384551130669\n",
      "Average loss epoch 14: 0.379007806207\n",
      "Average loss epoch 15: 0.374547844583\n",
      "Average loss epoch 16: 0.370369740959\n",
      "Average loss epoch 17: 0.366399738651\n",
      "Average loss epoch 18: 0.363041177844\n",
      "Average loss epoch 19: 0.359593847797\n",
      "Average loss epoch 20: 0.356826384654\n",
      "Average loss epoch 21: 0.353729384033\n",
      "Average loss epoch 22: 0.350896583848\n",
      "Average loss epoch 23: 0.348595708713\n",
      "Average loss epoch 24: 0.346321018669\n",
      "Average loss epoch 25: 0.344205058191\n",
      "Average loss epoch 26: 0.342341391287\n",
      "Average loss epoch 27: 0.340237156078\n",
      "Average loss epoch 28: 0.338583201378\n",
      "Average loss epoch 29: 0.336965921945\n",
      "Total time: 18.6086740494 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Test:39\n",
      "Test:40\n",
      "Test:41\n",
      "Test:42\n",
      "Test:43\n",
      "Test:44\n",
      "Test:45\n",
      "Test:46\n",
      "Test:47\n",
      "Test:48\n",
      "Test:49\n",
      "Test:50\n",
      "Test:51\n",
      "Test:52\n",
      "Test:53\n",
      "Test:54\n",
      "Test:55\n",
      "Test:56\n",
      "Test:57\n",
      "Test:58\n",
      "Test:59\n",
      "Test:60\n",
      "Test:61\n",
      "Test:62\n",
      "Test:63\n",
      "Test:64\n",
      "Test:65\n",
      "Test:66\n",
      "Test:67\n",
      "Test:68\n",
      "Test:69\n",
      "Test:70\n",
      "Test:71\n",
      "Test:72\n",
      "Test:73\n",
      "Test:74\n",
      "Test:75\n",
      "Test:76\n",
      "Test:77\n",
      "Accuracy 0.9116\n",
      "[88.28086519241333, 22.751082181930542, 18.608703136444092]\n",
      "[8, 64, 128]\n",
      "[0.92620000000000002, 0.91659999999999997, 0.91159999999999997]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tbatch_size_array.append(batch_size)\n",
    "\taccuracy_array.append(total_correct_preds/mnist.test.num_examples)\n",
    "\tprint execution_time_array\n",
    "\tprint batch_size_array\n",
    "\tprint accuracy_array\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.59100863532\n",
      "Average loss epoch 1: 0.983410337818\n",
      "Average loss epoch 2: 0.782786881255\n",
      "Average loss epoch 3: 0.682780609788\n",
      "Average loss epoch 4: 0.62159965807\n",
      "Average loss epoch 5: 0.579738357078\n",
      "Average loss epoch 6: 0.549131215315\n",
      "Average loss epoch 7: 0.525112637293\n",
      "Average loss epoch 8: 0.506023511987\n",
      "Average loss epoch 9: 0.490321216321\n",
      "Average loss epoch 10: 0.477108308645\n",
      "Average loss epoch 11: 0.465809070758\n",
      "Average loss epoch 12: 0.455924464163\n",
      "Average loss epoch 13: 0.447374545386\n",
      "Average loss epoch 14: 0.439672191566\n",
      "Average loss epoch 15: 0.432791333889\n",
      "Average loss epoch 16: 0.426713025319\n",
      "Average loss epoch 17: 0.421001553675\n",
      "Average loss epoch 18: 0.415532664996\n",
      "Average loss epoch 19: 0.411256131446\n",
      "Average loss epoch 20: 0.406874329269\n",
      "Average loss epoch 21: 0.402749941589\n",
      "Average loss epoch 22: 0.398843401921\n",
      "Average loss epoch 23: 0.395192640964\n",
      "Average loss epoch 24: 0.392122685909\n",
      "Average loss epoch 25: 0.388698176803\n",
      "Average loss epoch 26: 0.386039294372\n",
      "Average loss epoch 27: 0.383173359889\n",
      "Average loss epoch 28: 0.380442727392\n",
      "Average loss epoch 29: 0.378086496179\n",
      "Total time: 16.6226491928 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.9033\n",
      "[88.28086519241333, 22.751082181930542, 18.608703136444092, 16.622669219970703]\n",
      "[8, 64, 128, 256]\n",
      "[0.92620000000000002, 0.91659999999999997, 0.91159999999999997, 0.90329999999999999]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tbatch_size_array.append(batch_size)\n",
    "\taccuracy_array.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEPCAYAAABhkeIdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVmhHCxmZlYoB4uZmRXKwWJmZoX6/0G4RJ5DF6+3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67df35ed50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67dcb849d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('batch size')\n",
    "plt.ylabel('accuray')\n",
    "plt.plot (batch_size_array, accuracy_array, \"bo\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate_array = []\n",
    "accuracy_array_2 = []\n",
    "execution_time_array_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 2.1856682891\n",
      "Average loss epoch 1: 1.991021949\n",
      "Average loss epoch 2: 1.82673790577\n",
      "Average loss epoch 3: 1.6871477126\n",
      "Average loss epoch 4: 1.56847461417\n",
      "Average loss epoch 5: 1.46731051385\n",
      "Average loss epoch 6: 1.38045088933\n",
      "Average loss epoch 7: 1.30556854801\n",
      "Average loss epoch 8: 1.24086762032\n",
      "Average loss epoch 9: 1.1840812731\n",
      "Average loss epoch 10: 1.13416659275\n",
      "Average loss epoch 11: 1.09049865019\n",
      "Average loss epoch 12: 1.0510584414\n",
      "Average loss epoch 13: 1.01631662055\n",
      "Average loss epoch 14: 0.984708480467\n",
      "Average loss epoch 15: 0.956162339059\n",
      "Average loss epoch 16: 0.930245869906\n",
      "Average loss epoch 17: 0.906891780757\n",
      "Average loss epoch 18: 0.88487257885\n",
      "Average loss epoch 19: 0.864907923146\n",
      "Average loss epoch 20: 0.846819619431\n",
      "Average loss epoch 21: 0.829375026103\n",
      "Average loss epoch 22: 0.814030111115\n",
      "Average loss epoch 23: 0.798897449101\n",
      "Average loss epoch 24: 0.785547350612\n",
      "Average loss epoch 25: 0.772845564323\n",
      "Average loss epoch 26: 0.760748447659\n",
      "Average loss epoch 27: 0.748988984623\n",
      "Average loss epoch 28: 0.73804345337\n",
      "Average loss epoch 29: 0.728358242835\n",
      "Total time: 16.5312020779 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.8559\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array_2.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tlearning_rate_array.append(learning_rate)\n",
    "\taccuracy_array_2.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.86835663898\n",
      "Average loss epoch 1: 1.32666848475\n",
      "Average loss epoch 2: 1.06252861608\n",
      "Average loss epoch 3: 0.913274744125\n",
      "Average loss epoch 4: 0.818547560233\n",
      "Average loss epoch 5: 0.752463737938\n",
      "Average loss epoch 6: 0.703202912184\n",
      "Average loss epoch 7: 0.665299654007\n",
      "Average loss epoch 8: 0.635289339262\n",
      "Average loss epoch 9: 0.610439096238\n",
      "Average loss epoch 10: 0.589529791864\n",
      "Average loss epoch 11: 0.571540968162\n",
      "Average loss epoch 12: 0.55628836712\n",
      "Average loss epoch 13: 0.542863525938\n",
      "Average loss epoch 14: 0.530911268614\n",
      "Average loss epoch 15: 0.520604662249\n",
      "Average loss epoch 16: 0.510839561853\n",
      "Average loss epoch 17: 0.50189824458\n",
      "Average loss epoch 18: 0.49457634874\n",
      "Average loss epoch 19: 0.486779985445\n",
      "Average loss epoch 20: 0.480466646569\n",
      "Average loss epoch 21: 0.474257671109\n",
      "Average loss epoch 22: 0.468625563606\n",
      "Average loss epoch 23: 0.463211579858\n",
      "Average loss epoch 24: 0.458170898328\n",
      "Average loss epoch 25: 0.453423768839\n",
      "Average loss epoch 26: 0.44931791876\n",
      "Average loss epoch 27: 0.445411271442\n",
      "Average loss epoch 28: 0.441513753383\n",
      "Average loss epoch 29: 0.437897649045\n",
      "Total time: 16.4654409885 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.8929\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.005\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array_2.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tlearning_rate_array.append(learning_rate)\n",
    "\taccuracy_array_2.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.59055423124\n",
      "Average loss epoch 1: 0.983943474348\n",
      "Average loss epoch 2: 0.782968171846\n",
      "Average loss epoch 3: 0.682900744621\n",
      "Average loss epoch 4: 0.621636950803\n",
      "Average loss epoch 5: 0.579455578717\n",
      "Average loss epoch 6: 0.548591981424\n",
      "Average loss epoch 7: 0.524860900139\n",
      "Average loss epoch 8: 0.506038559931\n",
      "Average loss epoch 9: 0.490147902169\n",
      "Average loss epoch 10: 0.476671301177\n",
      "Average loss epoch 11: 0.465197319322\n",
      "Average loss epoch 12: 0.45567410669\n",
      "Average loss epoch 13: 0.447231315982\n",
      "Average loss epoch 14: 0.439147018105\n",
      "Average loss epoch 15: 0.432593030768\n",
      "Average loss epoch 16: 0.425939638063\n",
      "Average loss epoch 17: 0.420588181537\n",
      "Average loss epoch 18: 0.41545023414\n",
      "Average loss epoch 19: 0.410967842162\n",
      "Average loss epoch 20: 0.406103409618\n",
      "Average loss epoch 21: 0.402630919608\n",
      "Average loss epoch 22: 0.398479670565\n",
      "Average loss epoch 23: 0.394983438409\n",
      "Average loss epoch 24: 0.391734369865\n",
      "Average loss epoch 25: 0.388789839973\n",
      "Average loss epoch 26: 0.385678360465\n",
      "Average loss epoch 27: 0.382773367044\n",
      "Average loss epoch 28: 0.380541243843\n",
      "Average loss epoch 29: 0.377963704623\n",
      "Total time: 17.5128810406 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.9034\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array_2.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tlearning_rate_array.append(learning_rate)\n",
    "\taccuracy_array_2.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.28981293696\n",
      "Average loss epoch 1: 0.733157825804\n",
      "Average loss epoch 2: 0.600965766428\n",
      "Average loss epoch 3: 0.537022705947\n",
      "Average loss epoch 4: 0.498249946214\n",
      "Average loss epoch 5: 0.471453175088\n",
      "Average loss epoch 6: 0.451403551013\n",
      "Average loss epoch 7: 0.435946516902\n",
      "Average loss epoch 8: 0.423984667965\n",
      "Average loss epoch 9: 0.413363233766\n",
      "Average loss epoch 10: 0.404538378677\n",
      "Average loss epoch 11: 0.397172022646\n",
      "Average loss epoch 12: 0.39040199702\n",
      "Average loss epoch 13: 0.384784560059\n",
      "Average loss epoch 14: 0.379872345757\n",
      "Average loss epoch 15: 0.374620151297\n",
      "Average loss epoch 16: 0.370461242012\n",
      "Average loss epoch 17: 0.366443469424\n",
      "Average loss epoch 18: 0.363064062721\n",
      "Average loss epoch 19: 0.360153037393\n",
      "Average loss epoch 20: 0.35700030987\n",
      "Average loss epoch 21: 0.354372577829\n",
      "Average loss epoch 22: 0.351401376111\n",
      "Average loss epoch 23: 0.348938499865\n",
      "Average loss epoch 24: 0.346493235537\n",
      "Average loss epoch 25: 0.344771744283\n",
      "Average loss epoch 26: 0.34222027924\n",
      "Average loss epoch 27: 0.340685646528\n",
      "Average loss epoch 28: 0.338495628578\n",
      "Average loss epoch 29: 0.337047181606\n",
      "Total time: 17.3070189953 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.9114\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.02\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array_2.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tlearning_rate_array.append(learning_rate)\n",
    "\taccuracy_array_2.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 0.933667998448\n",
      "Average loss epoch 1: 0.530496872613\n",
      "Average loss epoch 2: 0.45788352275\n",
      "Average loss epoch 3: 0.422086687428\n",
      "Average loss epoch 4: 0.400255229986\n",
      "Average loss epoch 5: 0.38375501641\n",
      "Average loss epoch 6: 0.371956530715\n",
      "Average loss epoch 7: 0.363133790755\n",
      "Average loss epoch 8: 0.355587548185\n",
      "Average loss epoch 9: 0.348884386298\n",
      "Average loss epoch 10: 0.343335782465\n",
      "Average loss epoch 11: 0.338786425922\n",
      "Average loss epoch 12: 0.334381358994\n",
      "Average loss epoch 13: 0.331282531477\n",
      "Average loss epoch 14: 0.327182897828\n",
      "Average loss epoch 15: 0.324579036264\n",
      "Average loss epoch 16: 0.321704962822\n",
      "Average loss epoch 17: 0.319513689274\n",
      "Average loss epoch 18: 0.317029744665\n",
      "Average loss epoch 19: 0.315155027208\n",
      "Average loss epoch 20: 0.312656690793\n",
      "Average loss epoch 21: 0.31083188387\n",
      "Average loss epoch 22: 0.3095121094\n",
      "Average loss epoch 23: 0.308124184608\n",
      "Average loss epoch 24: 0.306185449326\n",
      "Average loss epoch 25: 0.30469509242\n",
      "Average loss epoch 26: 0.303392459647\n",
      "Average loss epoch 27: 0.302492158505\n",
      "Average loss epoch 28: 0.30095947694\n",
      "Average loss epoch 29: 0.299702851015\n",
      "Total time: 16.5716290474 seconds\n",
      "Optimization Finished!\n",
      "Test:0\n",
      "Test:1\n",
      "Test:2\n",
      "Test:3\n",
      "Test:4\n",
      "Test:5\n",
      "Test:6\n",
      "Test:7\n",
      "Test:8\n",
      "Test:9\n",
      "Test:10\n",
      "Test:11\n",
      "Test:12\n",
      "Test:13\n",
      "Test:14\n",
      "Test:15\n",
      "Test:16\n",
      "Test:17\n",
      "Test:18\n",
      "Test:19\n",
      "Test:20\n",
      "Test:21\n",
      "Test:22\n",
      "Test:23\n",
      "Test:24\n",
      "Test:25\n",
      "Test:26\n",
      "Test:27\n",
      "Test:28\n",
      "Test:29\n",
      "Test:30\n",
      "Test:31\n",
      "Test:32\n",
      "Test:33\n",
      "Test:34\n",
      "Test:35\n",
      "Test:36\n",
      "Test:37\n",
      "Test:38\n",
      "Accuracy 0.9182\n",
      "[0.001, 0.005, 0.01, 0.02, 0.05]\n",
      "[0.85589999999999999, 0.89290000000000003, 0.90339999999999998, 0.91139999999999999, 0.91820000000000002]\n"
     ]
    }
   ],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.05\n",
    "batch_size = 256\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint ('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "\tprint ('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\texecution_time_array_2.append(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\t\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tprint \"Test:\" + str(i)\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\n",
    "\tprint ('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "\tlearning_rate_array.append(learning_rate)\n",
    "\taccuracy_array_2.append(total_correct_preds/mnist.test.num_examples)\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.53122115135193, 16.4654598236084, 17.512901067733765, 17.307039976119995, 16.571649074554443]\n",
      "[0.001, 0.005, 0.01, 0.02, 0.05]\n",
      "[0.85589999999999999, 0.89290000000000003, 0.90339999999999998, 0.91139999999999999, 0.91820000000000002]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67d7289e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEPCAYAAACDTflkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67d7462dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print execution_time_array_2\n",
    "print learning_rate_array\n",
    "print accuracy_array_2\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot (learning_rate_array, accuracy_array_2, \"bo\")\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('execution time')\n",
    "plt.plot (learning_rate_array, execution_time_array_2, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
